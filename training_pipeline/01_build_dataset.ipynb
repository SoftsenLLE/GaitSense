{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OqNlA-7vIOZv"
   },
   "outputs": [],
   "source": [
    "# CELL 0 — IMPORTS, LOCATION\n",
    "import os, time, json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score, recall_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras import layers, models, optimizers, Sequential\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "import time, io\n",
    "from datetime import datetime\n",
    "\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/drive', force_remount=True)\n",
    "\n",
    "data_base = Path(\n",
    "    \"/content/drive/MyDrive/Colab Notebooks/classifier_2026_locomotion_mode/step1_labelled\")\n",
    "data_base.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "OUTPUT_ROOT = Path(\n",
    "    \"/content/drive/MyDrive/Colab Notebooks/classifier_2026_locomotion_mode/step3_models\")\n",
    "OUTPUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "DATASET_ROOT = Path(\n",
    "    \"/content/drive/MyDrive/Colab Notebooks/classifier_2026_locomotion_mode/step2_combined\")\n",
    "DATASET_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# print(os.listdir(data_base))\n",
    "\n",
    "subject_folders = sorted(\n",
    "  [d for d in os.listdir(data_base)\n",
    "    if d.lower().startswith(\"sub\")\n",
    "    ]\n",
    "  )\n",
    "assert len(subject_folders) > 1, \"Need at least 2 subjects for LOSO\"\n",
    "print(\"Subjects found:\", subject_folders)\n",
    "\n",
    "print('\\nCell 1 done at', datetime.now())  # useful for understanding which cell has been previously compiled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eUTIvJsKNExV"
   },
   "source": [
    "Min-Max Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2xv81AbdJ6BG"
   },
   "outputs": [],
   "source": [
    "# CELL 1.1 — Normalizae all CSVs using per-subject, per-sensor pre-trials min-max calibration routine data\n",
    "\n",
    "for sub in subject_folders:\n",
    "    sub_path = data_base / sub\n",
    "    print(f\"\\nProcessing {sub} ...\")\n",
    "\n",
    "    # Find normalization CSV\n",
    "    normal_files = list(sub_path.glob(\"*_normal.csv\"))\n",
    "\n",
    "    normal_path = normal_files[0]\n",
    "    normal_df = pd.read_csv(normal_path)\n",
    "\n",
    "    assert normal_df.shape[1] == 6, f\"{normal_path.name}: expected 6 columns\" # 6 sensors\n",
    "\n",
    "    # Reference min/max (column-wise)\n",
    "    col_min = normal_df.min()\n",
    "    col_max = normal_df.max()\n",
    "\n",
    "    # Min-Max scale all other CSVs for the subject\n",
    "    for csv_path in sub_path.glob(\"*.csv\"):\n",
    "        if csv_path == normal_path:\n",
    "            continue\n",
    "\n",
    "        df = pd.read_csv(csv_path)\n",
    "        assert df.shape[1] == 6, f\"{csv_path.name}: expected 6 columns\" # 6 sensors\n",
    "\n",
    "        df_norm = (df - col_min) / (col_max - col_min)\n",
    "        df_norm = df_norm.clip(0.0, 1.0) # safety clipping\n",
    "\n",
    "        # Overwrite original file\n",
    "        df_norm.to_csv(csv_path, index=False)\n",
    "\n",
    "    print(f\"{sub}: normalization complete\")\n",
    "\n",
    "\n",
    "print(\"\\nCell 2 done at\", datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UTTO_7mxN-9g"
   },
   "outputs": [],
   "source": [
    "# CELL 1.2 — Check if normalized CSVs are fine. If yes, delete the normalization CSV.\n",
    "\n",
    "for sub in subject_folders:\n",
    "    sub_path = data_base / sub\n",
    "    print(f\"\\nChecking {sub} ...\")\n",
    "\n",
    "    # Find normalization CSV\n",
    "    normal_files = list(sub_path.glob(\"*_normal.csv\"))\n",
    "    normal_path = normal_files[0]\n",
    "\n",
    "    # remove normalization CSV, not reqd. anymore\n",
    "    normal_path.unlink()\n",
    "\n",
    "\n",
    "print(\"\\nCell 3 done at\", datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jmRxVURdJwj3"
   },
   "source": [
    "During manual preprocessing, all subject data manually split into different activity-specific segmented csvs, using lap-counting stopwatch timestamps.\n",
    "\n",
    "Combine scaled CSVs of same class from selected subjects.\n",
    "\n",
    "The combined CSV built also shows the source subject data for manual verification. After all, correct data -> proper training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IUVY7L86IfSg"
   },
   "outputs": [],
   "source": [
    "# Cell 2 - Combine selected scaled datasets\n",
    "\n",
    "def cell2_combine_per_class(\n",
    "    subjects_to_use,\n",
    "    data_base,\n",
    "    combined_data_base,\n",
    "    tag\n",
    "):\n",
    "  print(\"Subjects used for combination:\")\n",
    "  for s in subjects_to_use:\n",
    "      print(\"  -\", s)\n",
    "\n",
    "  class_ids = set()\n",
    "\n",
    "  for sub in subjects_to_use:\n",
    "      for f in (data_base / sub).glob(f\"{sub}_*.csv\"):\n",
    "          class_id = f.stem.split(\"_\")[-1]\n",
    "          class_ids.add(class_id)\n",
    "\n",
    "  class_ids = sorted(class_ids)\n",
    "\n",
    "  print(\"\\nDetected class IDs:\")\n",
    "  for cid in class_ids:\n",
    "      print(\"  -\", cid)\n",
    "\n",
    "  if not class_ids:\n",
    "      raise RuntimeError(\"No class CSV files found.\")\n",
    "\n",
    "  for class_id in class_ids:\n",
    "      combined_df_list = []\n",
    "\n",
    "      for sub in subjects_to_use:\n",
    "          file_path = data_base / sub / f\"{sub}_{class_id}.csv\"\n",
    "          if not file_path.exists():\n",
    "              continue\n",
    "\n",
    "          df = pd.read_csv(file_path)\n",
    "          before_rows = len(df)\n",
    "\n",
    "          # Replace Inf with NaN, then drop rows with NaN\n",
    "          df = df.replace([np.inf, -np.inf], np.nan)\n",
    "          df = df.dropna()\n",
    "\n",
    "          after_rows = len(df)\n",
    "\n",
    "          if after_rows == 0:\n",
    "              print(f\"WARNING: {file_path} empty after cleanup\")\n",
    "              continue\n",
    "\n",
    "          df[\"source_subject\"] = sub\n",
    "          combined_df_list.append(df)\n",
    "\n",
    "          if before_rows != after_rows:\n",
    "              print(\n",
    "                  f\"Cleaned {file_path.name}: \"\n",
    "                  f\"dropped {before_rows - after_rows} rows\"\n",
    "              )\n",
    "\n",
    "      if not combined_df_list:\n",
    "          print(f\"Skipping class {class_id} (no valid data)\")\n",
    "          continue\n",
    "\n",
    "      combined_df = pd.concat(combined_df_list, ignore_index=True)\n",
    "\n",
    "      out_path = combined_data_base / f\"combined_class_{class_id}_{tag}.csv\"\n",
    "      combined_df.to_csv(out_path, index=False)\n",
    "\n",
    "      print(f\"Saved: {out_path} | rows = {len(combined_df)}\")\n",
    "\n",
    "  print(\"\\nCell 2 done at\", datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bfgkSWMFO72W"
   },
   "outputs": [],
   "source": [
    "# Cell 3 - User may confirm combined individual classes. Once confirmed, drop source_subject column\n",
    "\n",
    "def cell3_confirm_and_clean(combined_data_base, tag):\n",
    "    combined_files = sorted(combined_data_base.glob(f\"combined_class_*_{tag}.csv\"))\n",
    "\n",
    "    print(\"Combined class files detected:\")\n",
    "    for f in combined_files:\n",
    "        print(\"  -\", f.name)\n",
    "\n",
    "    for file_path in combined_files:\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        if \"source_subject\" in df.columns:\n",
    "            df = df.drop(columns=[\"source_subject\"])\n",
    "\n",
    "        df.to_csv(file_path, index=False)\n",
    "\n",
    "        class_id = file_path.stem.split(\"_\")[-1]\n",
    "        print(\n",
    "            f\"Processed {file_path.name}: \"\n",
    "            f\"rows={len(df)}, cols={df.shape[1]}\"\n",
    "        )\n",
    "\n",
    "    print(\"\\nCell 3 done at\", datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e2KbfeqGWOzx"
   },
   "source": [
    "Build segment wise combined dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n14htugwPU0k"
   },
   "outputs": [],
   "source": [
    "# Cell 4 - Build the First combined dataset\n",
    "\n",
    "def cell4_build_all_combined(\n",
    "    combined_data_base,\n",
    "    window_size,\n",
    "    step_distance,\n",
    "    tag\n",
    "):\n",
    "    def build_sequences_generic(df, time_steps, step, class_label):\n",
    "        segments = []\n",
    "        labels = []\n",
    "\n",
    "        values = df.values\n",
    "        num_features = values.shape[1]\n",
    "\n",
    "        for i in range(0, len(values) - time_steps, step):\n",
    "            segment = values[i:i + time_steps]\n",
    "            segments.append(segment)\n",
    "            labels.append(class_label)\n",
    "\n",
    "        X = np.asarray(segments, dtype=np.float32)\n",
    "        y = np.asarray(labels)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    combined_files = sorted(combined_data_base.glob(f\"combined_class_*_{tag}.csv\"))\n",
    "\n",
    "    all_rows = []\n",
    "    segment_offset = 0\n",
    "\n",
    "    print(\"Processing files and shapes:\\n\")\n",
    "\n",
    "    for file_path in combined_files:\n",
    "        stem_parts = file_path.stem.split(\"_\")\n",
    "\n",
    "        # Expected pattern:\n",
    "        # combined_class_<classID>_W.._S.._LOSO_subX\n",
    "        try:\n",
    "            class_idx = stem_parts.index(\"class\")\n",
    "            class_label = int(stem_parts[class_idx + 1])\n",
    "        except (ValueError, IndexError):\n",
    "            raise RuntimeError(\n",
    "                f\"Failed to parse class label from filename: {file_path.name}\"\n",
    "            )\n",
    "\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        print(file_path.name)\n",
    "        print(\"  Input CSV shape:\", df.shape)\n",
    "\n",
    "        X, y = build_sequences_generic(\n",
    "            df,\n",
    "            window_size,\n",
    "            step_distance,\n",
    "            class_label\n",
    "        )\n",
    "\n",
    "        print(\"  Segments shape (X):\", X.shape)\n",
    "        print(\"  Labels shape (y):\", y.shape)\n",
    "\n",
    "        for i in range(X.shape[0]):\n",
    "            for t in range(X.shape[1]):\n",
    "                row = [segment_offset + i, t]\n",
    "                row.extend(X[i, t].tolist())\n",
    "                row.append(y[i])\n",
    "                all_rows.append(row)\n",
    "\n",
    "        segment_offset += X.shape[0]\n",
    "\n",
    "    num_features = df.shape[1]\n",
    "    columns = (\n",
    "        [\"segment_id\", \"t\"]\n",
    "        + [f\"f{i}\" for i in range(num_features)]\n",
    "        + [\"label\"]\n",
    "    )\n",
    "\n",
    "    final_df = pd.DataFrame(all_rows, columns=columns)\n",
    "\n",
    "    output_path = combined_data_base / f\"all_combined_{tag}.csv\"\n",
    "    final_df.to_csv(output_path, index=False)\n",
    "\n",
    "    df_check = pd.read_csv(output_path)\n",
    "\n",
    "    nan_count = df_check.isna().sum().sum()\n",
    "    inf_count = np.isinf(df_check.select_dtypes(include=[np.number])).sum().sum()\n",
    "\n",
    "    print(\"\\n=== NaN / Inf CHECK: all_combined.csv ===\")\n",
    "    print(\"Shape:\", df_check.shape)\n",
    "    print(\"Total NaN values:\", nan_count)\n",
    "    print(\"Total Inf values:\", inf_count)\n",
    "\n",
    "    if nan_count == 0 and inf_count == 0:\n",
    "        print(\"OK: all_combined.csv has no NaN/Inf\")\n",
    "    else:\n",
    "        print(\"WARNING: NaN/Inf present\")\n",
    "\n",
    "    print(\"\\nFinal output:\")\n",
    "    print(\"  all_combined.csv shape:\", final_df.shape)\n",
    "    print(\"  Saved to:\", output_path)\n",
    "\n",
    "    print(\"\\nCell 4 done at\", datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Fq4MS41WTFk"
   },
   "source": [
    "Break into validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W0_H4yrvT5mz"
   },
   "outputs": [],
   "source": [
    "# Cell 5 - Build class-balanced validation dataset (segment-wise breaking)\n",
    "\n",
    "def cell5_build_validation(combined_data_base, tag, val_ratio=0.20):\n",
    "    input_path = combined_data_base / f\"all_combined_{tag}.csv\"\n",
    "    val_output_path = combined_data_base / f\"all_combined_val_{tag}.csv\"\n",
    "    train_output_path = combined_data_base / f\"all_combined_train_before_shuffle_{tag}.csv\"\n",
    "\n",
    "    df = pd.read_csv(input_path)\n",
    "\n",
    "    print(\"Initial dataset shape:\", df.shape)\n",
    "\n",
    "    val_segments = []\n",
    "    segments_to_drop = set()\n",
    "\n",
    "    for class_label, class_df in df.groupby(\"label\"):\n",
    "        print(f\"\\nProcessing class {class_label}\")\n",
    "\n",
    "        segment_ids = class_df[\"segment_id\"].unique()\n",
    "        num_segments = len(segment_ids)\n",
    "        num_val_segments = int(np.ceil(num_segments * val_ratio))\n",
    "\n",
    "        print(\"  Total segments:\", num_segments)\n",
    "        print(\"  Validation segments:\", num_val_segments)\n",
    "\n",
    "        segment_ids = segment_ids.copy()\n",
    "        np.random.shuffle(segment_ids)\n",
    "\n",
    "        selected_segments = segment_ids[:num_val_segments]\n",
    "\n",
    "        for seg_id in selected_segments:\n",
    "            seg_rows = df[df[\"segment_id\"] == seg_id]\n",
    "            val_segments.append(seg_rows)\n",
    "            segments_to_drop.add(seg_id)\n",
    "\n",
    "    val_df = pd.concat(val_segments, ignore_index=True)\n",
    "    val_df.to_csv(val_output_path, index=False)\n",
    "\n",
    "    print(\"\\nValidation set shape:\", val_df.shape)\n",
    "\n",
    "    train_df = df[~df[\"segment_id\"].isin(segments_to_drop)]\n",
    "    train_df.to_csv(train_output_path, index=False)\n",
    "\n",
    "    print(\"Remaining training set shape:\", train_df.shape)\n",
    "    print(\"\\nCell 5 done at\", datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KzfKbgEIWVdD"
   },
   "source": [
    "Optional shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uFsZvtxNT6me"
   },
   "outputs": [],
   "source": [
    "# Cell 6 - Window-wise shuffle of training data\n",
    "\n",
    "def cell6_shuffle_train(combined_data_base, tag):\n",
    "    input_path = combined_data_base / f\"all_combined_train_before_shuffle_{tag}.csv\"\n",
    "    output_path = combined_data_base / f\"all_combined_train_after_shuffle_{tag}.csv\"\n",
    "\n",
    "    df = pd.read_csv(input_path)\n",
    "\n",
    "    groups = [g for _, g in df.groupby(\"segment_id\")]\n",
    "    np.random.shuffle(groups)\n",
    "\n",
    "    shuffled_df = pd.concat(groups, ignore_index=True)\n",
    "    shuffled_df.to_csv(output_path, index=False)\n",
    "\n",
    "    print(\"Training data shuffled window-wise\")\n",
    "    print(\"Final training shape:\", shuffled_df.shape)\n",
    "    print(\"\\nCell 6 done at\", datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YPEzYs4eWYEz"
   },
   "source": [
    "Check if training data is correct in all aspects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i0k7fvtwT8qd"
   },
   "outputs": [],
   "source": [
    "# Cell 7 - Sanity checks for train/validation split\n",
    "\n",
    "def cell7_sanity_checks(combined_data_base, WINDOW_SIZE, tag, check_counter):\n",
    "\n",
    "  train_path = combined_data_base / f\"all_combined_train_after_shuffle_{tag}.csv\"\n",
    "  val_path   = combined_data_base / f\"all_combined_val_{tag}.csv\"\n",
    "\n",
    "  train_df = pd.read_csv(train_path)\n",
    "  val_df   = pd.read_csv(val_path)\n",
    "\n",
    "  print(\"=== BASIC SHAPES ===\")\n",
    "  print(\"Train shape:\", train_df.shape)\n",
    "  print(\"Val shape  :\", val_df.shape)\n",
    "  print(\"Total rows:\", train_df.shape[0] + val_df.shape[0])\n",
    "\n",
    "  # NaN / Inf checks\n",
    "  print(\"\\n=== NaN / Inf CHECKS ===\")\n",
    "\n",
    "  def nan_inf_check(df, name, check_counter):\n",
    "      nan_count = df.isna().sum().sum()\n",
    "      inf_count = np.isinf(df.select_dtypes(include=[np.number])).sum().sum()\n",
    "\n",
    "      print(f\"{name}:\")\n",
    "      print(f\"  Total NaN values : {nan_count}\")\n",
    "      print(f\"  Total Inf values : {inf_count}\")\n",
    "\n",
    "      if nan_count == 0 and inf_count == 0:\n",
    "          print(f\"  {name}: OK (no NaN / Inf detected)\")\n",
    "          check_counter += 1\n",
    "      else:\n",
    "          print(f\"  {name}: WARNING — NaN / Inf present\")\n",
    "\n",
    "          # Per-column breakdown (only columns with issues)\n",
    "          nan_cols = df.isna().sum()\n",
    "          nan_cols = nan_cols[nan_cols > 0]\n",
    "\n",
    "          inf_cols = np.isinf(df.select_dtypes(include=[np.number])).sum()\n",
    "          inf_cols = inf_cols[inf_cols > 0]\n",
    "\n",
    "          if not nan_cols.empty:\n",
    "              print(\"  Columns with NaNs:\")\n",
    "              print(nan_cols)\n",
    "\n",
    "          if not inf_cols.empty:\n",
    "              print(\"  Columns with Infs:\")\n",
    "              print(inf_cols)\n",
    "\n",
    "  nan_inf_check(train_df, \"Train\", check_counter)\n",
    "  nan_inf_check(val_df, \"Val\", check_counter)\n",
    "\n",
    "  # Segment integrity check\n",
    "  print(\"\\n=== SEGMENT INTEGRITY CHECK ===\")\n",
    "\n",
    "  def check_segment_sizes(df, name, check_counter):\n",
    "      counts = df.groupby(\"segment_id\").size()\n",
    "      bad = counts[counts != WINDOW_SIZE]\n",
    "      if len(bad) == 0:\n",
    "          print(f\"{name}: OK (all segments size = {WINDOW_SIZE})\")\n",
    "          check_counter += 1\n",
    "      else:\n",
    "          print(f\"{name}: FAILED\")\n",
    "          print(bad.head())\n",
    "\n",
    "  check_segment_sizes(train_df, \"Train\", check_counter)\n",
    "  check_segment_sizes(val_df, \"Val\", check_counter)\n",
    "\n",
    "  # Segment data leakage check\n",
    "  print(\"\\n=== LEAKAGE CHECK (segment overlap) ===\")\n",
    "\n",
    "  train_segments = set(train_df[\"segment_id\"].unique())\n",
    "  val_segments   = set(val_df[\"segment_id\"].unique())\n",
    "\n",
    "  overlap = train_segments.intersection(val_segments)\n",
    "\n",
    "  print(\"Train segments:\", len(train_segments))\n",
    "  print(\"Val segments  :\", len(val_segments))\n",
    "  print(\"Segment Overlap       :\", len(overlap))\n",
    "\n",
    "  if len(overlap) == 0:\n",
    "      print(\"OK: No segment leakage\")\n",
    "      check_counter += 1\n",
    "  else:\n",
    "      print(\"FAILED: Overlapping segment_ids detected\")\n",
    "\n",
    "  # Class distribution (rows)\n",
    "  print(\"\\n=== CLASS DISTRIBUTION (rows) ===\")\n",
    "\n",
    "  train_class_counts = train_df[\"label\"].value_counts().sort_index()\n",
    "  val_class_counts   = val_df[\"label\"].value_counts().sort_index()\n",
    "\n",
    "  class_dist = pd.DataFrame({\n",
    "      \"train_rows\": train_class_counts,\n",
    "      \"val_rows\": val_class_counts,\n",
    "      \"val_ratio\": val_class_counts / (train_class_counts + val_class_counts)\n",
    "  })\n",
    "\n",
    "  print(class_dist)\n",
    "\n",
    "  # Class distribution (segments)\n",
    "  print(\"\\n=== CLASS DISTRIBUTION (segments) ===\")\n",
    "\n",
    "  train_seg_counts = train_df.groupby(\"label\")[\"segment_id\"].nunique()\n",
    "  val_seg_counts   = val_df.groupby(\"label\")[\"segment_id\"].nunique()\n",
    "\n",
    "  seg_dist = pd.DataFrame({\n",
    "      \"train_segments\": train_seg_counts,\n",
    "      \"val_segments\": val_seg_counts\n",
    "  })\n",
    "\n",
    "  print(seg_dist)\n",
    "\n",
    "  # Window order check\n",
    "  print(\"\\n=== WINDOW ORDER CHECK ===\")\n",
    "\n",
    "  def check_order(df, name, check_counter):\n",
    "      bad = False\n",
    "      for seg_id, seg_df in df.groupby(\"segment_id\"):\n",
    "          if not np.all(seg_df[\"t\"].values == np.arange(WINDOW_SIZE)):\n",
    "              bad = True\n",
    "              print(f\"{name}: ordering issue in segment {seg_id}\")\n",
    "              break\n",
    "      if not bad:\n",
    "          print(f\"{name}: OK (t ordering preserved)\")\n",
    "          check_counter += 1\n",
    "\n",
    "  check_order(train_df, \"Train\", check_counter)\n",
    "  check_order(val_df, \"Val\", check_counter)\n",
    "\n",
    "  # Column consistency\n",
    "  print(\"\\n=== COLUMN CONSISTENCY ===\")\n",
    "\n",
    "  if list(train_df.columns) == list(val_df.columns):\n",
    "      print(\"OK: Train and Val columns identical\")\n",
    "      check_counter += 1\n",
    "  else:\n",
    "      print(\"FAILED: Column mismatch\")\n",
    "\n",
    "  print(\"\\n=== FINAL SUMMARY ===\")\n",
    "\n",
    "  # Update the condition to reflect the actual number of checks (2 for NaN/Inf, 2 for segment sizes, 1 for leakage, 2 for order, 1 for columns = 8 checks)\n",
    "  print(\"\\ncheck count: \", check_counter)\n",
    "  if check_counter == 8:\n",
    "      print(\"\\nOK: All checks passed :-)\")\n",
    "  elif check_counter < 8 and check_counter > 0:\n",
    "      print(\"! Failed ! : Some checks failed\")\n",
    "  else:\n",
    "      print(\"!!! FAILED !!!: No checks passed\")\n",
    "\n",
    "  print(\"\\nTrain rows:\", len(train_df))\n",
    "  print(\"Val rows  :\", len(val_df))\n",
    "  print(\"Val ratio :\", len(val_df) / (len(train_df) + len(val_df)))\n",
    "\n",
    "  print(\"\\nCell 7 done at\", datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tA0tl2yhWcMP"
   },
   "source": [
    "Loop over multi-subjecs and multi window-stride combos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BHxYB5SmT-uC"
   },
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CELL 8 — DATASET MATERIALIZATION ORCHESTRATOR\n",
    "#\n",
    "# Build and freeze datasets ONCE:\n",
    "#  - For each (WINDOW_SIZE, STEP_DISTANCE)\n",
    "#  - For each LOSO subject\n",
    "#  - Calls Cell 2 to Cell 7\n",
    "# ============================================================\n",
    "\n",
    "# ----------------------------\n",
    "# CONFIGURATION\n",
    "# ----------------------------\n",
    "\n",
    "WINDOW_STRIDE_CONFIGS = [\n",
    "    (100, 20),\n",
    "    (100, 50),\n",
    "    (50, 10),\n",
    "    (50, 25)\n",
    "]\n",
    "\n",
    "SUBJECTS = sorted(\n",
    "    [d for d in os.listdir(data_base) if d.lower().startswith(\"sub\")]\n",
    ")\n",
    "SUBJECTS = [\n",
    "    \"sub1\",\n",
    "    \"sub2\",\n",
    "    # \"sub3\", # bad data\n",
    "    \"sub4\",\n",
    "    \"sub5\",\n",
    "    \"sub6\",\n",
    "    \"sub7\",\n",
    "    \"sub8\",\n",
    "    \"sub9\",\n",
    "    \"sub10\",\n",
    "    \"sub11\",\n",
    "]\n",
    "\n",
    "print(\"\\n========== DATASET MATERIALIZATION ==========\")\n",
    "print(\"Subjects detected:\")\n",
    "for s in SUBJECTS:\n",
    "    print(\"  -\", s)\n",
    "\n",
    "print(\"\\nWindow–Stride combinations:\")\n",
    "for W, S in WINDOW_STRIDE_CONFIGS:\n",
    "    print(f\"  - W{W}, S{S}\")\n",
    "\n",
    "print(\"\\nDataset root:\")\n",
    "print(\" \", DATASET_ROOT)\n",
    "\n",
    "print(\"\\nStarting dataset creation...\\n\")\n",
    "\n",
    "for WINDOW_SIZE, STEP_DISTANCE in WINDOW_STRIDE_CONFIGS:\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"WINDOW = {WINDOW_SIZE}, STRIDE = {STEP_DISTANCE}\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    for TEST_SUBJECT in SUBJECTS:\n",
    "        start_time = time.time()\n",
    "\n",
    "        print(\"\\n\" + \"-\" * 70)\n",
    "        print(f\"LOSO SUBJECT: {TEST_SUBJECT}\")\n",
    "        print(\"-\" * 70)\n",
    "\n",
    "        # TAG (used in ALL filenames)\n",
    "        tag = f\"W{WINDOW_SIZE}_S{STEP_DISTANCE}_LOSO_{TEST_SUBJECT}\"\n",
    "        # print(\"Tag:\", tag)\n",
    "\n",
    "        # OUTPUT DIRECTORY FOR THIS DATASET\n",
    "        combined_data_base = (\n",
    "            DATASET_ROOT\n",
    "            / f\"W{WINDOW_SIZE}_S{STEP_DISTANCE}\"\n",
    "            / f\"LOSO_{TEST_SUBJECT}\"\n",
    "        )\n",
    "\n",
    "        combined_data_base.mkdir(parents=True, exist_ok=True)\n",
    "        if not combined_data_base.is_dir():\n",
    "            raise RuntimeError(f\"Failed to create directory: {combined_data_base}. Please check Google Drive connection and permissions.\")\n",
    "\n",
    "        print(\"\\n[Step 1] Dataset directory found:\")\n",
    "        print(\" \", combined_data_base)\n",
    "\n",
    "        # SUBJECTS USED FOR TRAINING\n",
    "        subjects_to_use = [s for s in SUBJECTS if s != TEST_SUBJECT]\n",
    "\n",
    "        print(\"Training subjects:\")\n",
    "        for s in subjects_to_use:\n",
    "            print(\"  -\", s)\n",
    "\n",
    "        # =====================================================\n",
    "        # CELL 2 — COMBINE PER-CLASS DATA\n",
    "        # =====================================================\n",
    "        print(\"\\n[Step 2] Combining per-class data\")\n",
    "        cell2_combine_per_class(\n",
    "            subjects_to_use=subjects_to_use,\n",
    "            data_base=data_base,\n",
    "            combined_data_base=combined_data_base,\n",
    "            tag=tag\n",
    "        )\n",
    "\n",
    "        # =====================================================\n",
    "        # CELL 3 — CONFIRM & CLEAN COMBINED CSVs\n",
    "        # =====================================================\n",
    "        print(\"\\n[Step 3] Confirming and cleaning combined CSVs\")\n",
    "        cell3_confirm_and_clean(\n",
    "            combined_data_base=combined_data_base,\n",
    "            tag=tag\n",
    "        )\n",
    "\n",
    "        # =====================================================\n",
    "        # CELL 4 — BUILD all_combined CSV\n",
    "        # =====================================================\n",
    "        print(\"\\n[Step 4] Building all_combined CSV\")\n",
    "        cell4_build_all_combined(\n",
    "            combined_data_base=combined_data_base,\n",
    "            window_size=WINDOW_SIZE,\n",
    "            step_distance=STEP_DISTANCE,\n",
    "            tag=tag\n",
    "        )\n",
    "\n",
    "        # =====================================================\n",
    "        # CELL 5 — BUILD TRAIN / VAL SPLIT\n",
    "        # =====================================================\n",
    "        print(\"\\n[Step 5] Building train / validation split\")\n",
    "        cell5_build_validation(\n",
    "            combined_data_base=combined_data_base,\n",
    "            # window_size=WINDOW_SIZE,\n",
    "            tag=tag,\n",
    "            val_ratio=0.20  # 20% validation split\n",
    "        )\n",
    "\n",
    "        # =====================================================\n",
    "        # CELL 6 — SHUFFLE TRAIN DATA (SEGMENT-WISE)\n",
    "        # =====================================================\n",
    "        print(\"\\n[Step 6] Shuffling training data (segment-wise)\")\n",
    "        cell6_shuffle_train(\n",
    "            combined_data_base=combined_data_base,\n",
    "            tag=tag\n",
    "        )\n",
    "\n",
    "        # =====================================================\n",
    "        # CELL 7 — SANITY CHECKS\n",
    "        # =====================================================\n",
    "        print(\"\\n[Step 7] Running sanity checks\")\n",
    "        check_counter = 0\n",
    "        cell7_sanity_checks(\n",
    "            combined_data_base=combined_data_base,\n",
    "            WINDOW_SIZE=WINDOW_SIZE,\n",
    "            tag=tag,\n",
    "            check_counter=check_counter\n",
    "        )\n",
    "\n",
    "        print(\"\\n[Cleanup] Removing intermediate CSV files\")\n",
    "\n",
    "        ### Check if correct data exists:\n",
    "        required_files = []\n",
    "\n",
    "        # Per-class combined CSVs\n",
    "        for cls in range(7):\n",
    "            required_files.append(\n",
    "                combined_data_base / f\"combined_class_{cls:02d}_{tag}.csv\"\n",
    "            )\n",
    "\n",
    "        # Final frozen datasets\n",
    "        required_files.extend([\n",
    "            combined_data_base / f\"all_combined_val_{tag}.csv\",\n",
    "            combined_data_base / f\"all_combined_train_after_shuffle_{tag}.csv\",\n",
    "        ])\n",
    "\n",
    "        missing = []\n",
    "        for f in required_files:\n",
    "            if not f.exists():\n",
    "                missing.append(f.name)\n",
    "\n",
    "        if missing:\n",
    "            print(\"\\n!!! DATASET VERIFICATION FAILED !!!\")\n",
    "            print(\"Missing required files:\")\n",
    "            for m in missing:\n",
    "                print(\"  -\", m)\n",
    "            raise RuntimeError(\n",
    "                f\"Dataset integrity check failed for {tag}. \"\n",
    "                f\"Missing {len(missing)} required CSV(s).\"\n",
    "            )\n",
    "        else:\n",
    "            print(\"\\n** Dataset verification successful **\")\n",
    "\n",
    "\n",
    "        ### Delete redundant data\n",
    "        files_to_delete = [\n",
    "            combined_data_base / f\"all_combined_{tag}.csv\",\n",
    "            # combined_data_base / f\"all_combined_train_before_shuffle_{tag}.csv\",\n",
    "        ]\n",
    "        for cls in range(7):\n",
    "            files_to_delete.append(\n",
    "                combined_data_base / f\"combined_class_{cls:02d}_{tag}.csv\"\n",
    "            )\n",
    "\n",
    "        for f in files_to_delete:\n",
    "            if f.exists():\n",
    "                f.unlink()\n",
    "                print(f\"  Deleted: {f.name}\")\n",
    "            else:\n",
    "                print(f\"  Skipped (not found): {f.name}\")\n",
    "\n",
    "\n",
    "\n",
    "        print(\"\\n Dataset generation COMPLETE for:\", tag)\n",
    "        print(\"\\n Time taken:\", time.time() - start_time, \"seconds\")\n",
    "\n",
    "print(\"\\n******** ALL DATASETS BUILT SUCCESSFULLY ********\")\n",
    "print(\"\\nCell 8 done at\", datetime.now())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNY4bVhMSZ0XtLy5ytbUUxg",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
