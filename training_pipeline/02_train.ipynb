{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNASARdrnA2VSjr0PUPPaic"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Zge5kRguW7lC"},"outputs":[],"source":["# CELL 1 — IMPORTS, LOCATION\n","import os, time, json\n","import pandas as pd\n","import numpy as np\n","from pathlib import Path\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.utils.class_weight import compute_class_weight\n","from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, f1_score, recall_score\n","import matplotlib.pyplot as plt\n","\n","import tensorflow as tf\n","from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n","from tensorflow.keras import layers, models, optimizers, Sequential\n","\n","from tensorflow.keras.layers import Dense, Dropout\n","from tensorflow.keras.optimizers import Adam\n","\n","import time, io\n","from datetime import datetime\n","\n","from google.colab import drive\n","\n","drive.mount('/content/drive', force_remount=True)\n","\n","data_base = Path(\n","    \"/content/drive/MyDrive/Colab Notebooks/classifier_2026_locomotion_mode/step1_labelled\")\n","data_base.mkdir(parents=True, exist_ok=True)\n","\n","OUTPUT_ROOT = Path(\n","    \"/content/drive/MyDrive/Colab Notebooks/classifier_2026_locomotion_mode/step3_models\")\n","OUTPUT_ROOT.mkdir(parents=True, exist_ok=True)\n","\n","\n","DATASET_ROOT = Path(\n","    \"/content/drive/MyDrive/Colab Notebooks/classifier_2026_locomotion_mode/step2_combined\")\n","DATASET_ROOT.mkdir(parents=True, exist_ok=True)\n","\n","# print(os.listdir(data_base))\n","\n","subject_folders = sorted(\n","    [d for d in os.listdir(data_base)\n","      if d.lower().startswith(\"sub\")\n","      ]\n","    )\n","assert len(subject_folders) > 1, \"Need at least 2 subjects for LOSO\"\n","print(\"Subjects found:\", subject_folders)\n","\n","print('\\nCell 1 done at', datetime.now())  # useful for understanding which cell has been previously compiled"]},{"cell_type":"code","source":["# CELL 2 — Data loading (TRAIN + VAL only), tensor building, augmentation, class weights\n","\n","# CSV → (N, T, F) ----------------\n","def csv_to_tensor(df, window_size):\n","    feature_cols = [c for c in df.columns if c.startswith(\"f\")]\n","    X, y = [], []\n","\n","    for seg_id, seg_df in df.groupby(\"segment_id\"):\n","        seg_df = seg_df.sort_values(\"t\")\n","        if len(seg_df) != window_size:\n","            continue  # safety\n","        X.append(seg_df[feature_cols].values) # T, F\n","        y.append(seg_df[\"label\"].iloc[0])     # Scalar\n","\n","    X = np.asarray(X, dtype=np.float32)\n","    y = np.asarray(y)\n","    return X, y\n","\n","def augment_raw(X, y_ohe):\n","      \"\"\"\n","      X: (N, timesteps*window_size, num_sensors) in [0,1]\n","      Augmentation:\n","        - per-sample, per-sensor scale + offset\n","        - additive Gaussian noise\n","      Returns augmented X, y_ohe (doubled in size).\n","      \"\"\"\n","      X_aug = X.copy()\n","      N, T, S = X.shape\n","      y_int = np.argmax(y_ohe, axis=1)   # shape (N,)\n","\n","      for i in range(N):\n","          Xi = X_aug[i]\n","          cls = y_int[i]\n","\n","          scale_range=(0.75, 1.25)\n","          offset_range=(-0.25, 0.25)\n","          noise_std=0.05\n","\n","          # --- per-sensor scale and offset ---\n","          scales = np.random.uniform(scale_range[0], scale_range[1], size=(1, S))\n","          offsets = np.random.uniform(offset_range[0], offset_range[1], size=(1, S))\n","          Xi = Xi * scales + offsets\n","\n","          # --- Gaussian noise ---\n","          Xi = Xi + np.random.normal(0.0, noise_std, size=(T, S))\n","\n","          # clip back to [0, 1]\n","          X_aug[i] = np.clip(Xi, 0.0, 1.0)\n","\n","      X_out = np.concatenate([X, X_aug], axis=0)\n","      y_out = np.concatenate([y_ohe, y_ohe], axis=0)\n","      return X_out, y_out\n","\n","def load_train_val_tensors(dataset_root, tag, window_size, num_classes):\n","    # train_path = dataset_root / f\"all_combined_train_after_shuffle_{tag}.csv\"\n","    train_path = dataset_root / f\"all_combined_train_before_shuffle_{tag}.csv\"\n","    val_path   = dataset_root / f\"all_combined_val_{tag}.csv\"\n","\n","    print(f\"\\nLoading TRAIN: {train_path.name}\")\n","    print(f\"Loading VAL  : {val_path.name}\")\n","\n","    train_df = pd.read_csv(train_path)\n","    val_df   = pd.read_csv(val_path)\n","\n","    X_train, y_train = csv_to_tensor(train_df, window_size)\n","    X_val,   y_val   = csv_to_tensor(val_df, window_size)\n","\n","    y_train_oh = tf.keras.utils.to_categorical(y_train, num_classes)\n","    y_val_oh   = tf.keras.utils.to_categorical(y_val, num_classes)\n","\n","    # Data Augmentation\n","    X_train, y_train_oh = augment_raw(X_train, y_train_oh)\n","\n","    # ---------------- Class weights (AFTER augmentation) ----------------\n","    y_train_aug = np.argmax(y_train_oh, axis=1)\n","    class_weights_arr = compute_class_weight(\n","        class_weight=\"balanced\",\n","        classes=np.arange(num_classes),\n","        y=y_train_aug\n","    )\n","    class_weights = {i: float(class_weights_arr[i]) for i in range(num_classes)}\n","\n","    class_weights = None\n","\n","    print(\"Class weights:\", class_weights)\n","\n","\n","\n","    return X_train, y_train_oh, X_val, y_val_oh, class_weights\n","\n","print('\\nCell 2 done at', datetime.now())"],"metadata":{"id":"xvBgLeaGW_ab"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# CELL 3 — Build & compile CNN model\n","\n","# Models expect:\n","#   input_shape = (T, F)\n","#\n","#   T = window_size\n","#   F = number of sensor data / features, per timestep (column-wise)\n","\n","\n","def build_cnn_model(input_shape, num_classes, k1, k2):\n","    inp = tf.keras.layers.Input(shape=input_shape, name=\"raw_input\")\n","    print(\"raw input shape ->\", inp.shape)\n","\n","    x = tf.keras.layers.Conv1D(32, kernel_size=k1, padding=\"same\",\n","                      activation=\"relu\", name=\"conv1\")(inp)\n","    # x = tf.keras.layers.MaxPooling1D(pool_size=2, name=\"max-pool\")(x)\n","    x = tf.keras.layers.Conv1D(64, kernel_size=k2, padding=\"same\",\n","                      activation=\"relu\", name=\"conv2\")(x)\n","    x = tf.keras.layers.MaxPooling1D(pool_size=2, name=\"maxpool\")(x)\n","\n","    z = tf.keras.layers.GlobalAveragePooling1D(name=\"gap\")(x)\n","    # z = tf.keras.layers.Flatten(name=\"flatten\")(x)\n","    z = tf.keras.layers.Dense(32, activation=\"relu\", name=\"dense1\")(z)\n","    z = tf.keras.layers.Dropout(0.3, name=\"drop1\")(z)\n","\n","    out = tf.keras.layers.Dense(num_classes, activation=\"softmax\", name=\"output\")(z)\n","\n","    model = models.Model(inp, out, name=\"cnn_1d_model\")\n","\n","    model.summary()\n","    model.compile(\n","        optimizer=optimizers.Adam(learning_rate=1e-3),  # standard Adam\n","        loss=\"categorical_crossentropy\",\n","        metrics=[\n","            'accuracy',\n","            'mse',\n","            tf.keras.metrics.Precision(name=\"precision\")\n","            ]\n","    )\n","\n","    return model\n","\n","print('\\nCell 3 done at', datetime.now())"],"metadata":{"id":"yraJ1_X3XHln"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# CELL 4 — Train model and save .keras\n","MAX_EPOCHS = 200\n","\n","def train_and_save_model(\n","    model,\n","    X_train, y_train_oh,\n","    X_val, y_val_oh,\n","    class_weights,\n","    batch_size,\n","    model_dir,\n","    model_name\n","):\n","    callbacks = [\n","        EarlyStopping(\n","            monitor=\"val_loss\",\n","            patience=25,  #10-15% of MAX_EPOCHS\n","            min_delta=0.01,\n","            mode=\"min\",\n","            restore_best_weights=False,\n","            verbose=1\n","        ),\n","        ReduceLROnPlateau(\n","            monitor=\"val_loss\",\n","            factor=0.5,\n","            patience=13,  # ~ half of early stop pateince\n","            min_delta=0.01,\n","            mode=\"min\",\n","            min_lr=1e-4,  # prevent too low LR, may overfit intra-subject\n","            verbose=1\n","        )\n","    ]\n","\n","    history = model.fit(\n","        X_train, y_train_oh,\n","        validation_data=(X_val, y_val_oh),\n","        epochs=MAX_EPOCHS,\n","        batch_size=batch_size,\n","        class_weight=class_weights,\n","        callbacks=callbacks,\n","        verbose=1\n","    )\n","\n","    model_dir.mkdir(parents=True, exist_ok=True)\n","    keras_path = model_dir / f\"{model_name}.keras\"\n","    model.save(keras_path)\n","\n","    saved_model_dir = model_dir / \"saved_model\"\n","    model.export(saved_model_dir)\n","\n","    print(f\"Saved Keras model: {keras_path.name}\")\n","    return keras_path, saved_model_dir\n","\n","print('\\nCell 4 done at', datetime.now())"],"metadata":{"id":"N4IrJBAjXVja"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# CELL 5 — Convert .keras to INT8 TFLite\n","\n","def convert_to_int8_tflite(saved_model_dir, X_train, tflite_save_path):\n","    # model = tf.keras.models.load_model(keras_path)\n","\n","    # Randomize train set and provide a representative data for int8 qunatization calibration\n","    idx = np.random.choice(len(X_train), size=min(200, len(X_train)), replace=False)\n","    def representative_dataset():\n","        for i in idx:\n","            yield [X_train[i:i+1].astype(np.float32)]\n","\n","    converter = tf.lite.TFLiteConverter.from_saved_model(str(saved_model_dir))\n","    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n","    converter.representative_dataset = representative_dataset\n","    converter.target_spec.supported_ops = [\n","        tf.lite.OpsSet.TFLITE_BUILTINS_INT8,\n","        # tf.lite.OpsSet.TFLITE_SELECT_OPS,\n","        ]\n","    converter.inference_input_type = tf.int8\n","    converter.inference_output_type = tf.int8\n","\n","    tflite_model = converter.convert()\n","\n","    with open(tflite_save_path, \"wb\") as f:\n","        f.write(tflite_model)\n","\n","    print(f\"Saved TFLite model: {tflite_save_path.name}\")\n","    return tflite_save_path\n","\n","print('\\nCell 5 done at', datetime.now())"],"metadata":{"id":"4WerxaHAXY-1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# CELL 6 — Evaluate validation set using TFLite only\n","\n","def evaluate_tflite_model(tflite_path, X, y_true):\n","    interpreter = tf.lite.Interpreter(model_path=str(tflite_path))\n","    interpreter.allocate_tensors()\n","\n","    input_details  = interpreter.get_input_details()[0]\n","    output_details = interpreter.get_output_details()[0]\n","\n","    scale, zero = input_details[\"quantization\"]\n","\n","    y_pred = []\n","\n","    for i in range(X.shape[0]):\n","        x = X[i:i+1]\n","        x_q = np.clip(\n","            np.round(x / scale + zero),\n","            -128, 127\n","        ).astype(np.int8)\n","\n","        interpreter.set_tensor(input_details[\"index\"], x_q)\n","        interpreter.invoke()\n","\n","        out = interpreter.get_tensor(output_details[\"index\"])\n","        y_pred.append(np.argmax(out))\n","\n","    y_pred = np.array(y_pred)\n","\n","    cm_raw = confusion_matrix(y_true, y_pred)\n","\n","    cm_norm = cm_raw.astype(np.float32)\n","    row_sums = cm_norm.sum(axis=1, keepdims=True)\n","    cm_norm = np.divide(cm_norm, row_sums, where=row_sums != 0)\n","\n","    return {\n","        \"accuracy\": accuracy_score(y_true, y_pred),\n","        \"f1_macro\": f1_score(y_true, y_pred, average=\"macro\"),\n","        \"cm_raw\": cm_raw,\n","        \"cm_norm\": cm_norm,\n","    }\n","\n","print('\\nCell 6 done at', datetime.now())"],"metadata":{"id":"h7MCQG_rXhzS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# CELL 7 — Build LOSO TEST dataset (combine + segment, no shuffle)\n","\n","def build_loso_test_tensor(\n","    subject_folder,\n","    window_size,\n","    step_distance\n","):\n","    \"\"\"\n","    Build LOSO test dataset from raw subject CSVs\n","    - combine all class files\n","    - segment using window_size and step_distance\n","    - NO shuffling\n","    \"\"\"\n","\n","    segments = []\n","    labels = []\n","\n","    print(f\"\\nBuilding LOSO test set from: {subject_folder.name}\")\n","    print(f\"Window size: {window_size}, Step distance: {step_distance}\")\n","\n","    for csv_file in sorted(subject_folder.glob(\"*.csv\")):\n","        df = pd.read_csv(csv_file)\n","        class_label = int(csv_file.stem.split(\"_\")[-1])\n","\n","        values = df.values  # (N, F)\n","\n","        for start in range(0, len(values) - window_size + 1, step_distance):\n","            segment = values[start:start + window_size]\n","            segments.append(segment)\n","            labels.append(class_label)\n","\n","    X_test = np.asarray(segments, dtype=np.float32)\n","    y_test = np.asarray(labels)\n","\n","    print(\"LOSO test tensor shape:\", X_test.shape)\n","    print(\"LOSO test labels shape:\", y_test.shape)\n","\n","    return X_test, y_test\n","\n","\n","print('\\nCell 7 done at', datetime.now())"],"metadata":{"id":"gOmIriK_Xr7T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plot_and_save_confusion_matrix(\n","    cm,\n","    title,\n","    save_path,\n","    cmap=\"Greens\",\n","    dpi=450\n","):\n","    \"\"\"\n","    Parameters\n","    ----------\n","    cm : np.ndarray\n","        Normalized confusion matrix of shape (NUM_CLASSES, NUM_CLASSES),\n","        values in [0, 1]\n","    title : str\n","        Figure title\n","    save_path : Path or str\n","        Where to save the PNG\n","    cmap : str\n","        Matplotlib colormap\n","    dpi : int\n","        Resolution for publication\n","    \"\"\"\n","\n","    cm = np.asarray(cm, dtype=np.float32)\n","    num_classes = cm.shape[0]\n","\n","    fig, ax = plt.subplots(figsize=(7, 6), dpi=dpi)\n","    im = ax.imshow(cm, interpolation=\"nearest\", cmap=cmap, vmin=0, vmax=1)\n","\n","    ax.set_title(title, fontsize=14)\n","    ax.set_xlabel(\"Predicted label\", fontsize=12)\n","    ax.set_ylabel(\"True label\", fontsize=12)\n","\n","    ax.set_xticks(np.arange(num_classes))\n","    ax.set_yticks(np.arange(num_classes))\n","    ax.set_xticklabels(np.arange(num_classes))\n","    ax.set_yticklabels(np.arange(num_classes))\n","\n","    # Colorbar\n","    cbar = fig.colorbar(im, ax=ax, fraction=0.046)\n","    cbar.set_label(\"Normalized value\", fontsize=11)\n","\n","    # Cell annotations\n","    thresh = 0.5\n","    for i in range(num_classes):\n","        for j in range(num_classes):\n","            ax.text(\n","                j, i,\n","                f\"{cm[i, j]:.2f}\",\n","                ha=\"center\",\n","                va=\"center\",\n","                fontsize=11,\n","                color=\"white\" if cm[i, j] > thresh else \"black\"\n","            )\n","\n","    plt.tight_layout()\n","\n","    save_path = Path(save_path)\n","    save_path.parent.mkdir(parents=True, exist_ok=True)\n","    plt.savefig(save_path, bbox_inches=\"tight\")\n","    plt.close(fig)\n","\n","    print(f\"Saved confusion matrix → {save_path}\")\n","\n","print('\\nCell 7 done at', datetime.now())"],"metadata":{"id":"cOQwRFYnX2qY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# CELL 15 — PARAMETRIC SWEEP + LOSO + AGGREGATION\n","\n","NUM_CLASSES = 7\n","\n","# ---------------- CONFIGURATIONS ----------------\n","\n","WINDOW_STRIDE_CONFIGS = [\n","    (100, 20),\n","    (100, 50),\n","    (50, 10),\n","    (50, 25),\n","]\n","\n","KERNEL_CONFIGS = [\n","    (11, 7),\n","    (9, 5),\n","    # (7, 7),\n","    (7, 5),\n","    # (7, 3),\n","    # (5, 3),\n","    # (5, 3)\n","]\n","\n","BATCH_SIZES = [\n","    # 16,\n","    32,\n","    64\n","    ]\n","\n","SUBJECTS = [\n","    \"sub1\",\n","    \"sub2\",\n","    # \"sub3\", # bad data\n","    \"sub4\",\n","    \"sub5\",\n","    \"sub6\",\n","    \"sub7\",\n","    \"sub8\",\n","    \"sub9\",\n","    \"sub10\",\n","    \"sub11\",\n","]\n","\n","print(\"\\n========== PARAMETRIC STUDY SETUP ==========\")\n","print(\"NUM_CLASSES           :\", NUM_CLASSES)\n","print(\"Window–Stride configs :\", WINDOW_STRIDE_CONFIGS)\n","print(\"Kernel configs        :\", KERNEL_CONFIGS)\n","print(\"Batch sizes           :\", BATCH_SIZES)\n","print(\"LOSO subjects         :\", SUBJECTS)\n","print(\"===========================================\\n\")\n","\n","# ---------------- MASTER LOOP ----------------\n","\n","for (W, S) in WINDOW_STRIDE_CONFIGS:\n","    for (k1, k2) in KERNEL_CONFIGS:\n","        for batch_size in BATCH_SIZES:\n","            start_time = time.time()\n","            print(\"\\n\" + \"#\" * 100)\n","            print(\n","                f\"START CONFIG → W{W}_S{S} | kernels=({k1},{k2}) | batch={batch_size}\"\n","            )\n","            print(\"#\" * 100)\n","\n","            # Store per-configuration results\n","            CONFIG_RESULTS   = []\n","            CONFIG_CMS_RAW   = []\n","            CONFIG_CMS_NORM  = []\n","\n","            for loso_sub in SUBJECTS:\n","\n","                tag = f\"W{W}_S{S}_LOSO_{loso_sub}\"\n","\n","                dataset_root = Path(\"/content/drive/MyDrive/Colab Notebooks/classifier_2026_locomotion_mode/step2_combined\")\n","                dataset_root = (\n","                    dataset_root\n","                    / f\"W{W}_S{S}\"\n","                    / f\"LOSO_{loso_sub}\"\n","                )\n","                if not dataset_root.exists():\n","                  raise FileNotFoundError(f\"Dataset root does not exist: {dataset_root}\")\n","\n","                print(\"\\n\" + \"-\" * 80)\n","                print(f\"LOSO SUBJECT → {loso_sub}\")\n","                print(f\"Dataset root → {dataset_root}\")\n","                print(\"-\" * 80)\n","\n","                # =====================================================\n","                # CELL 2 — Load TRAIN + VAL tensors\n","                # =====================================================\n","                X_train, y_train_oh, X_val, y_val_oh, class_weights = \\\n","                    load_train_val_tensors(\n","                        dataset_root=dataset_root,\n","                        tag=tag,\n","                        window_size=W,\n","                        num_classes=NUM_CLASSES\n","                    )\n","\n","                # =====================================================\n","                # CELL 3 — Build model\n","                # =====================================================\n","                model = build_cnn_model(\n","                    input_shape=(W, X_train.shape[2]),\n","                    num_classes=NUM_CLASSES,\n","                    k1=k1,\n","                    k2=k2\n","                )\n","\n","                model_name = f\"cnn_model_{tag}_k{k1}-{k2}_batch{batch_size}\"\n","\n","                model_dir = (\n","                    OUTPUT_ROOT\n","                    / f\"W{W}_S{S}\"\n","                    / f\"k{k1}-{k2}\"\n","                    / f\"batch{batch_size}\"\n","                    / f\"LOSO_{loso_sub}\"\n","                )\n","                model_dir.mkdir(parents=True, exist_ok=True)\n","\n","                # cm_dir = model_dir / \"confusion_matrices\"\n","                # cm_dir.mkdir(parents=True, exist_ok=True)\n","\n","                # =====================================================\n","                # CELL 4 — Train + save .keras\n","                # =====================================================\n","                _, saved_model_dir = train_and_save_model(\n","                    model=model,\n","                    X_train=X_train,\n","                    y_train_oh=y_train_oh,\n","                    X_val=X_val,\n","                    y_val_oh=y_val_oh,\n","                    # class_weights=class_weights,\n","                    class_weights=None,\n","                    batch_size=batch_size,\n","                    model_dir=model_dir,\n","                    model_name=model_name\n","                )\n","                train_tm = time.time() - start_time\n","                eval_strt_tm = time.time()\n","                print(\"\\nTime taken to train: \", train_tm)\n","                print(\" secs\")\n","                # =====================================================\n","                # CELL 5 — Convert to INT8 TFLite\n","                # =====================================================\n","                tflite_path = model_dir / f\"{model_name}.tflite\"\n","                convert_to_int8_tflite(saved_model_dir, X_train, tflite_path)\n","\n","                # =====================================================\n","                # CELL 6 — Validation (TFLite only)\n","                # =====================================================\n","                val_metrics = evaluate_tflite_model(\n","                    tflite_path=tflite_path,\n","                    X=X_val,\n","                    y_true=np.argmax(y_val_oh, axis=1),\n","                    # normalize_cm=True\n","                )\n","\n","                # =====================================================\n","                # CELL 7 — LOSO TEST (build + evaluate)\n","                # =====================================================\n","                subject_folder = data_base / loso_sub\n","\n","                X_test, y_test = build_loso_test_tensor(\n","                    subject_folder=subject_folder,\n","                    window_size=W,\n","                    step_distance=S\n","                )\n","\n","                test_metrics = evaluate_tflite_model(\n","                    tflite_path=tflite_path,\n","                    X=X_test,\n","                    y_true=y_test,\n","                    # normalize_cm=True\n","                )\n","\n","                eval_tm = time.time() - eval_strt_tm\n","                print(\"\\nTime taken to eval: \", eval_tm)\n","                print(\" secs\")\n","\n","                # -----------------------------------------------------\n","                # Store per-fold results\n","                # -----------------------------------------------------\n","                print(\n","                    f\"[VAL ] acc={val_metrics['accuracy']:.3f}, \"\n","                    f\"f1={val_metrics['f1_macro']:.3f}\"\n","                )\n","                print(\n","                    f\"[TEST] acc={test_metrics['accuracy']:.3f}, \"\n","                    f\"f1={test_metrics['f1_macro']:.3f}\"\n","                )\n","\n","                CONFIG_RESULTS.append({\n","                    \"loso_subject\": loso_sub,\n","                    \"val_accuracy\": val_metrics[\"accuracy\"],\n","                    \"val_f1_macro\": val_metrics[\"f1_macro\"],\n","                    \"test_accuracy\": test_metrics[\"accuracy\"],\n","                    \"test_f1_macro\": test_metrics[\"f1_macro\"],\n","                })\n","\n","                CONFIG_CMS_RAW.append(test_metrics[\"cm_raw\"])\n","                CONFIG_CMS_NORM.append(test_metrics[\"cm_norm\"])\n","\n","                # -----------------------------------------------------\n","                # Save per-fold confusion matrices\n","                # -----------------------------------------------------\n","                plot_and_save_confusion_matrix(\n","                    cm=test_metrics[\"cm_raw\"],\n","                    title=f\"{model_name} — LOSO {loso_sub} (Counts)\",\n","                    save_path=model_dir / \"cm_raw.png\"\n","                )\n","\n","                plot_and_save_confusion_matrix(\n","                    cm=test_metrics[\"cm_norm\"],\n","                    title=f\"{model_name} — LOSO {loso_sub} (Normalized)\",\n","                    save_path=model_dir / \"cm_norm.png\"\n","                )\n","\n","            # =====================================================\n","            # AGGREGATION — PER CONFIGURATION\n","            # =====================================================\n","            config_df = pd.DataFrame(CONFIG_RESULTS)\n","\n","            mean_vals = config_df.mean(numeric_only=True)\n","            std_vals  = config_df.std(numeric_only=True)\n","\n","            summary_df = pd.concat(\n","                [\n","                    config_df,\n","                    pd.DataFrame([{\n","                        \"loso_subject\": \"MEAN\",\n","                        **mean_vals.to_dict()\n","                    }]),\n","                    pd.DataFrame([{\n","                        \"loso_subject\": \"STD\",\n","                        **std_vals.to_dict()\n","                    }]),\n","                ],\n","                ignore_index=True\n","            )\n","\n","            # Save summary\n","            summary_dir = (\n","                OUTPUT_ROOT\n","                / f\"W{W}_S{S}\"\n","                / f\"k{k1}-{k2}\"\n","                / f\"batch{batch_size}\"\n","            )\n","            summary_dir.mkdir(parents=True, exist_ok=True)\n","\n","            summary_path = summary_dir / \"summary_results.xlsx\"\n","            summary_df.to_excel(summary_path, index=False)\n","\n","            print(\"\\nSaved summary →\", summary_path)\n","\n","            # =====================================================\n","            # Average normalized confusion matrix\n","            # =====================================================\n","            avg_cm_raw  = np.sum(np.stack(CONFIG_CMS_RAW, axis=0), axis=0)\n","            avg_cm_norm = np.mean(np.stack(CONFIG_CMS_NORM, axis=0), axis=0)\n","\n","            plot_and_save_confusion_matrix(\n","                cm=avg_cm_raw,\n","                title=f\"Avg Confusion Matrix (Counts)\\nW{W}_S{S}, k{k1}-{k2}, batch{batch_size}\",\n","                save_path=summary_dir / \"avg_cm_raw.png\"\n","            )\n","\n","            plot_and_save_confusion_matrix(\n","                cm=avg_cm_norm,\n","                title=f\"Avg Confusion Matrix (Normalized)\\nW{W}_S{S}, k{k1}-{k2}, batch{batch_size}\",\n","                save_path=summary_dir / \"avg_cm_norm.png\"\n","            )\n","\n","            print(\n","                f\"Completed CONFIG → \"\n","                f\"W{W}_S{S} | k{k1}-{k2} | batch{batch_size}\"\n","            )\n","\n","\n","print('\\nCell 8 done at', datetime.now())"],"metadata":{"id":"Oe5ywOpzYI5U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"iQMqIc8UYulp"},"execution_count":null,"outputs":[]}]}